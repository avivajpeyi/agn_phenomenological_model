{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running population analysis with `GWPopulation`\n",
    "\n",
    "First, before running this you should install `gwpopulation` and `cupy` if you have access to a Nvidia GPU.\n",
    "\n",
    "```\n",
    "pip install gwpopulation\n",
    "pip install cupy\n",
    "```\n",
    "\n",
    "## `GWPopulation`\n",
    "\n",
    "`gwpopulation` is a library for performing GPU-accelerated population inference with a focus on gravitational wave populations.\n",
    "\n",
    "If `cupy` is installed the GPU will be used, if not the performance will suffer but it will fall back to `numpy`.\n",
    "\n",
    "Builds upon [`bilby`](git.ligo.org/lscsoft/bilby) ([arXiv:1811.02042](https://arxiv.org/abs/1811.02042)) to provide simple, modular, user-friendly, population inference.\n",
    "\n",
    "Currently implemented models include:\n",
    "- One and two component mass distributions in primary mass and mass ratio, e.g., Talbot & Thrane (2018) ([arXiv:1801:02699](https://arxiv.org/abs/1801.02699)), Fishbach & Holz (2018) ([arXiv:1709.08584](https://arxiv.org/abs/1709.08584)).\n",
    "- The same mass distributions but independent but identically distributed primary and secondary.\n",
    "- Half-Gaussian + isotropic spin tilt distribution from Talbot & Thrane (2017) ([arXiv:1704.08370](https://arxiv.org/abs/1704.08370)).\n",
    "- Beta spin magnitude distribution from Wysocki+ (2018) ([arXiv:1805:06442](https://arxiv.org/abs/1805.06442)).\n",
    "- Each of these are also available with independent but identically distributed spins.\n",
    "- Redshift evolution model as in Fishbach+ (2018) ([arXiv:1805.10270](https://arxiv.org/abs/1805.10270)).\n",
    "- More to come and any contributions welcome...\n",
    "\n",
    "For more information see the [git repository](https://github.com/ColmTalbot/gwpopulation), [documentation](https://colmtalbot.github.io/gwpopulation/), or [paper](https://dcc.ligo.org/LIGO-P1900101).\n",
    "\n",
    "If you want to try out a GPU-accelerated version you can open [this notebook](https://github.com/ColmTalbot/gwpopulation/blob/master/examples/GWTC1.ipynb) in [colab](colab.research.google.com).\n",
    "It runs through a similar demonstration without accessing any proprietary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import h5py\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d, RegularGridInterpolator\n",
    "import deepdish as dd\n",
    "from astropy import cosmology, units\n",
    "\n",
    "import bilby as bb\n",
    "from bilby.core.prior import LogUniform, PriorDict, Uniform\n",
    "from bilby.hyper.model import Model\n",
    "import gwpopulation as gwpop\n",
    "xp = gwpop.cupy_utils.xp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load posteriors\n",
    "\n",
    "We're using the posteriors from the GWTC-1 data release.\n",
    "\n",
    "We need to change the names of the parameters to make them work with the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_translator = dict(\n",
    "    mass_1_det='m1_detector_frame_Msun',\n",
    "    mass_2_det='m2_detector_frame_Msun',\n",
    "    luminosity_distance='luminosity_distance_Mpc',\n",
    "    a_1='spin1',\n",
    "    a_2='spin2',\n",
    "    cos_tilt_1='costilt1',\n",
    "    cos_tilt_2='costilt2')\n",
    "\n",
    "posteriors = list()\n",
    "priors = list()\n",
    "\n",
    "events = ['150914', '151012', '151226', '170104', '170608',\n",
    "          '170729', '170809', '170814', '170818', '170823']\n",
    "for event in events:\n",
    "\n",
    "    _posterior = pd.DataFrame()\n",
    "    _prior = pd.DataFrame()\n",
    "    with h5py.File('/home/colm.talbot/event_posteriors/GWTC1/GWTC-1_sample_release/GW{}_GWTC-1.hdf5'.format(event)) as ff:\n",
    "        for my_key, gwtc_key in parameter_translator.items():\n",
    "            _posterior[my_key] = ff['IMRPhenomPv2_posterior'][gwtc_key]\n",
    "            _prior[my_key] = ff['prior'][gwtc_key]\n",
    "    posteriors.append(_posterior)\n",
    "    priors.append(_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luminosity_distances = np.linspace(1, 10000, 1000)\n",
    "redshifts = np.array([\n",
    "    cosmology.z_at_value(cosmology.Planck15.luminosity_distance, dl * units.Mpc)\n",
    "    for dl in luminosity_distances])\n",
    "dl_to_z = interp1d(luminosity_distances, redshifts)\n",
    "\n",
    "luminosity_prior = luminosity_distances ** 2\n",
    "\n",
    "dz_ddl = np.gradient(redshifts, luminosity_distances)\n",
    "\n",
    "redshift_prior = interp1d(redshifts, luminosity_prior / dz_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add some weights to posterior\n",
    "\n",
    "Make sure the posterior `DataFrames` contain the appropriate quantities.\n",
    "\n",
    "We could include a `prior` column, this is the prior used in the initial sampling stage.\n",
    "This is used to weight the samples in the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for posterior in posteriors:\n",
    "    posterior['redshift'] = dl_to_z(posterior['luminosity_distance'])\n",
    "    posterior['mass_1'] = posterior['mass_1_det'] / (1 + posterior['redshift'])\n",
    "    posterior['mass_2'] = posterior['mass_2_det'] / (1 + posterior['redshift'])\n",
    "    posterior['mass_ratio'] = posterior['mass_2'] / posterior['mass_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the model\n",
    "\n",
    "Choose which population models we want to use, here we'll use mass model B from the populations paper, a powerlaw mass distribution with powerlaw mass ratio distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([gwpop.models.mass.power_law_primary_mass_ratio])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_vt_data = dd.io.load('/home/colm.talbot/O2/population/paper_runs/vt.h5')\n",
    "vt_data = dict()\n",
    "vt_data['mass_1'] = _vt_data['m1']\n",
    "vt_data['mass_ratio'] = _vt_data['q']\n",
    "vt_data['vt'] = _vt_data['vt_early_high'] * _vt_data['quadratic_calibration']\n",
    "vt_evaluator = gwpop.vt.GridVT(\n",
    "    model=gwpop.models.mass.power_law_primary_mass_ratio, data=vt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the likelihood\n",
    "\n",
    "In this notebook we use a likelihood which analytically marginalises over the local merger rate assuming a uniform-in-log prior.\n",
    "If you want to estimate the rate use the `RateLikelihood` and add `rate` as a parameter.\n",
    "This is done further on in the notebook.\n",
    "\n",
    "We provide:\n",
    "- `posteriors`: a list of `pandas` DataFrames\n",
    "- `hyper_prior`: our population model, as defined above\n",
    "- `sampling_prior`: this is being depracated\n",
    "- `selection_function`: anything which evaluates the selection function\n",
    "\n",
    "We can also provide:\n",
    "- `max_samples`: the maximum number of samples to use from each posterior, this defaults to the length of the shortest posterior\n",
    "- `conversion_function`: a function which takes the sampled parameter dictionary and returns a dictionary also containing the parameters required for the population model.\n",
    "\n",
    "We get a warning telling us `cupy` is not available and so `numpy` is for the likelihood evaluation.\n",
    "This will go away if you have a GPU and `cupy` installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_likelihood = gwpop.hyperpe.HyperparameterLikelihood(\n",
    "    posteriors=posteriors, hyper_prior=model, selection_function=vt_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the prior\n",
    "\n",
    "This is the standard method to define the prior distribution within `bilby`.\n",
    "\n",
    "The labels are used in plotting.\n",
    "\n",
    "Numbers are converted to delta function priors and are not sampled.\n",
    "\n",
    "There are many other distributions available, see the code/documentation for a full list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_priors = PriorDict()\n",
    "\n",
    "fast_priors['alpha'] = Uniform(minimum=-2, maximum=4, latex_label='$\\\\alpha$')\n",
    "fast_priors['beta'] = Uniform(minimum=-4, maximum=12, latex_label='$\\\\beta$')\n",
    "fast_priors['mmin'] = Uniform(minimum=5, maximum=10, latex_label='$m_{\\\\min}$')\n",
    "fast_priors['mmax'] = Uniform(minimum=20, maximum=60, latex_label='$m_{\\\\max}$')\n",
    "fast_priors['lam'] = 0\n",
    "fast_priors['mpp'] = 35\n",
    "fast_priors['sigpp'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the sampler\n",
    "\n",
    "We'll use the sampler `dynesty` and use a small number of live points.\n",
    "\n",
    "This is painfully slow without using the GPU version.\n",
    "If you have a GPU it will just work.\n",
    "\n",
    "On jupyter.ligo.caltech.edu the CPU version ran in ~30 minutes.\n",
    "On colab.research.google.com this ran in a GPU runtime is ~30 seconds.\n",
    "\n",
    "Other samplers are available, `cpnest` gave the best results for the O1+O2 data, however it isn't currently compatible with the GPU likelihood.\n",
    "\n",
    "_Note_:\n",
    "With this few live points the sampler might throw some warnings.\n",
    "They'll go away with more sensible settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_result = bb.run_sampler(\n",
    "    likelihood=fast_likelihood, priors=fast_priors, sampler='dynesty', nlive=100,\n",
    "    label='fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = fast_result.plot_corner(save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a new model\n",
    "\n",
    "### Let's define a new population model for BNS. \n",
    "\n",
    "Just as an example we'll use a Gaussian distribution bounded between $[1 M_{\\odot}, 2 M_{\\odot}]$.\n",
    "\n",
    "$$p(m_1, m_2) = N \\exp \\left(- \\frac{\\left((m_1 - \\mu)^2 + (m_2 - \\mu)^2\\right)}{2 \\sigma^2}\\right) \\quad : \\quad 1 \\leq m_2 \\leq m_1 \\leq 2$$\n",
    "\n",
    "We see that this function takes three arguments:\n",
    "- `dataset`: this is common to all of the population models in `gwpopulation`, it is a dictionary containing the data to be evaluated, here it is assumed to contain entries for `mass_1` and `mass_2`, the _source-frame_ masses.\n",
    "- `mu_bns`: the peak of the bns mass distribution.\n",
    "- `sigma_bns`: the width of the bns mass distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_gaussian_primary_secondary_identical(dataset, mu_bns, sigma_bns):\n",
    "    prob = gwpop.utils.truncnorm(\n",
    "        dataset['mass_1'], mu=mu_bns, sigma=sigma_bns, low=1, high=2)\n",
    "    prob *= gwpop.utils.truncnorm(\n",
    "        dataset['mass_2'], mu=mu_bns, sigma=sigma_bns, low=1, high=2)\n",
    "    prob *= (dataset['mass_1'] >= dataset['mass_2'])\n",
    "    prob *= 2\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GW170817 posterior\n",
    "\n",
    "This is just the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = pd.DataFrame()\n",
    "prior = pd.DataFrame()\n",
    "with h5py.File('/home/colm.talbot/event_posteriors/GWTC1/GWTC-1_sample_release/GW170817_GWTC-1.hdf5') as ff:\n",
    "    for my_key, gwtc_key in parameter_translator.items():\n",
    "        try:\n",
    "            posterior[my_key] = ff['IMRPhenomPv2NRT_lowSpin_posterior'][gwtc_key]\n",
    "            prior[my_key] = ff['IMRPhenomPv2NRT_lowSpin_prior'][gwtc_key]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "posterior['redshift'] = dl_to_z(posterior['luminosity_distance'])\n",
    "posterior['mass_1'] = posterior['mass_1_det'] / (1 + posterior['redshift'])\n",
    "posterior['mass_2'] = posterior['mass_2_det'] / (1 + posterior['redshift'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the new likelihood\n",
    "\n",
    "We use the same likelihood as before.\n",
    "\n",
    "_Note_:\n",
    "- This time we cast our posterior to a list while creating the likelihood.\n",
    "- We pass the function rather than a `Model` object as before, `bilby` will turn this into a `Model` for internal use.\n",
    "- We've removed the selection and conversion functions as they aren't needed here (yes, a selection function is techinically needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bns_likelihood = gwpop.hyperpe.HyperparameterLikelihood(\n",
    "    posteriors=[posterior], hyper_prior=truncated_gaussian_primary_secondary_identical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the new prior\n",
    "\n",
    "Just as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bns_priors = PriorDict()\n",
    "bns_priors['mu_bns'] = Uniform(minimum=1, maximum=2, latex_label='$\\\\mu_{bns}$')\n",
    "bns_priors['sigma_bns'] = LogUniform(minimum=1e-2, maximum=1, latex_label='$\\\\sigma_{bns}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run sampler\n",
    "\n",
    "The sampler is run as before.\n",
    "\n",
    "We've changed the:\n",
    "- `label`\n",
    "- `nlive`, number of live points, this run is faster, so we can use more live points in a quick run\n",
    "\n",
    "Unsurprisingly, the posterior peaks slightly at the equal mass limit of 170817 but essentially returns the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bns_result = bb.run_sampler(\n",
    "    likelihood=bns_likelihood, priors=bns_priors, sampler='dynesty',\n",
    "    nlive=1000, label='bns')\n",
    "\n",
    "_ = bns_result.plot_corner(save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do it all\n",
    "\n",
    "#### Let's put together a run with models for the mass, spin and redshift distributions.\n",
    "\n",
    "This will take longer to run and requires a model for VT including the effect of redshift evolution.\n",
    "\n",
    "_Note_:\n",
    "- the redshift model is a class and so is called slightly differently. This is to enable caching of expensive data internally.\n",
    "- this currently doesn't work with the pip release of `bilby`, will be fixed this week (v0.4.2), currently works with `master`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = Model([\n",
    "    gwpop.models.mass.two_component_primary_mass_ratio,\n",
    "    gwpop.models.spin.iid_spin_magnitude_beta,\n",
    "    gwpop.models.spin.independent_spin_orientation_gaussian_isotropic,\n",
    "    gwpop.models.redshift.PowerLawRedshift()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update sampling prior\n",
    "\n",
    "We need to update the sampling prior to account for the new redshift evolution model.\n",
    "\n",
    "Fortunately, we defined an interpolant for this earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for posterior in posteriors:\n",
    "    posterior['prior'] *= redshift_prior(posterior['redshift'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redshift dependent VT\n",
    "\n",
    "Data from Maya\n",
    "\n",
    "This again uses the `GridVT` with a 3D grid in primary mass, mass ratio, and redshift.\n",
    "\n",
    "I apply a constant factor, this was found by brute force to make the rate look right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt_file = '/home/colm.talbot/O2/population/data/O2_populations_maya/data/PDETS_GstLAL-PSDs.hdf5'\n",
    "    \n",
    "pdet_redshift = dd.io.load(vt_file)\n",
    "pdet_vt_interp = RegularGridInterpolator(\n",
    "    (pdet_redshift['ms'], pdet_redshift['ms'], pdet_redshift['zs']),\n",
    "    pdet_redshift['int_pdet_L1'], bounds_error=False, fill_value=0)\n",
    "m1s = np.linspace(5.95, 100, 200)\n",
    "qs = np.linspace(0.01, 1, 100)\n",
    "zs = np.linspace(1e-6, 1, 40)\n",
    "\n",
    "m1_grid, q_grid, z_grid = np.meshgrid(m1s, qs, zs)\n",
    "pdets = pdet_vt_interp(np.array([m1_grid * q_grid, m1_grid, z_grid]).T).T\n",
    "pdet_dict = dict(\n",
    "    mass_1=m1_grid, mass_ratio=q_grid, redshift=z_grid, vt=pdets)\n",
    "for key in pdet_dict:\n",
    "    pdet_dict[key] = np.asarray(pdet_dict[key])\n",
    "z_grid_vt = gwpop.vt.GridVT(\n",
    "    model=[gwpop.models.mass.two_component_primary_mass_ratio,\n",
    "           gwpop.models.redshift.PowerLawRedshift()], data=pdet_dict)\n",
    "\n",
    "def vt_eval_z_grid(parameters):\n",
    "    return z_grid_vt(parameters) / 0.040109860595670524    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the likelihood\n",
    "\n",
    "Here we use the likelihood which also estimates the rate.\n",
    "\n",
    "We actually lose our resolving power when we include the redshift evolution parameter.\n",
    "\n",
    "We add an extra argument:\n",
    "- `conversion_function`: this converts between the parameters we sample in and those needed by the model, e.g., for sampling in the mean and variance of the beta distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_likelihood = gwpop.hyperpe.RateLikelihood(\n",
    "    posteriors=posteriors, hyper_prior=full_model, selection_function=vt_eval_z_grid,\n",
    "    conversion_function=gwpop.conversions.convert_to_beta_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_priors = PriorDict()\n",
    "\n",
    "# rate\n",
    "full_priors['rate'] = LogUniform(minimum=1e-20, maximum=1e20, latex_label='$R$')\n",
    "# mass\n",
    "full_priors['alpha'] = Uniform(minimum=-4, maximum=12, latex_label='$\\\\alpha$')\n",
    "full_priors['beta'] = Uniform(minimum=-4, maximum=12, latex_label='$\\\\beta$')\n",
    "full_priors['mmin'] = Uniform(minimum=5, maximum=10, latex_label='$m_{\\\\min}$')\n",
    "full_priors['mmax'] = Uniform(minimum=20, maximum=60, latex_label='$m_{\\\\max}$')\n",
    "full_priors['lam'] = Uniform(minimum=0, maximum=1, latex_label='$\\\\lambda_{m}$')\n",
    "full_priors['mpp'] = Uniform(minimum=20, maximum=50, latex_label='$\\\\mu_{m}$')\n",
    "full_priors['sigpp'] = Uniform(minimum=0, maximum=10, latex_label='$\\\\sigma_{m}$')\n",
    "# spin magnitude\n",
    "full_priors['amax'] = 1\n",
    "full_priors['alpha_chi'] = Uniform(minimum=-4, maximum=12, latex_label='$\\\\alpha_{\\\\chi}$')\n",
    "full_priors['beta_chi'] = Uniform(minimum=-4, maximum=12, latex_label='$\\\\beta_{\\\\chi}$')\n",
    "# spin orientation\n",
    "full_priors['xi_spin'] = Uniform(minimum=0, maximum=1, latex_label='$\\\\xi$')\n",
    "full_priors['sigma_1'] = Uniform(minimum=0, maximum=4, latex_label='$\\\\sigma{1}$')\n",
    "full_priors['sigma_2'] = Uniform(minimum=0, maximum=4, latex_label='$\\\\sigma{2}$')\n",
    "# redshift evolution\n",
    "full_priors['lamb'] = Uniform(minimum=-25, maximum=25, latex_label='$\\\\lambda_{z}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run sampler\n",
    "\n",
    "This will take a while... use a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_result = bb.run_sampler(\n",
    "    likelihood=full_likelihood, priors=full_priors, sampler='dynesty',\n",
    "    nlive=1000, label='full')\n",
    "\n",
    "_ = full_result.plot_corner(save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Colm 3.6",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
